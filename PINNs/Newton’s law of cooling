# THE EQUATION IS dT/dt=R(T_env-T) WITH T(t) AS THE FUNCTION

import torch
import torch.nn as nn
import torch.optim as optim
import functools
import matplotlib as mpl ; mpl.rcParams['text.usetex'] = True # to use latex rendering in the legends and titles
import matplotlib.pyplot as plt
import numpy as np
import torch
import math
import copy
import time
import matplotlib.gridspec as gridspec



torch.manual_seed(42)
np.random.seed(10)


# DEFINE THE GROUND TRUTH FUNCTION
def cooling_law(time, T_env, T_0, R):
    T = T_env + (T_0 - T_env) * np.exp(-R * time)
    return T

# SET THE PARAMETERS OF THE PROBLEM
T_env = 25
T_0 = 100
R = 0.005
times = np.linspace(0, 1000, 1000) # set the time domain
eq = functools.partial(cooling_law, T_env=T_env, T_0=T_0, R=R) #  ???????????????????????????????????????????
T_truth = eq(times) # compute the ground thruth on the time domain

""" 
plt.plot(times, T_truth, alpha=1,c='k') 
plt.xlabel('Time (s)')
plt.title('Analytical solution of Newton\'s cooling law')
plt.show()
 """
# CREATE A FUNCTION TO TURN A LIST INTO TORCH TENSOR
def np_to_th(x):
    n_samples = len(x)
    return torch.from_numpy(x).to(torch.float).reshape(n_samples, -1).requires_grad_(True)


# CONSTRUCT THE NEURAL NETWORK
class Net(nn.Module):
    def __init__(
        self,
        input_dim, # dimension of the input layer
        output_dim, # dimension of the output layer
        n_units=100, # number of neurones in each hidden layer
    ) -> None: #  ?????????????????????????????????????????????????????????????????????????????????????????? 
        super().__init__() #  ??????????????????????????????????????????????????????????????????????????????
        self.n_units = n_units
        self.layers = nn.Sequential(
            nn.Linear(input_dim, self.n_units),
            nn.ReLU(),
            nn.Linear(self.n_units, self.n_units),
            nn.ReLU(),
            nn.Linear(self.n_units, self.n_units),
            nn.ReLU(),
            nn.Linear(self.n_units, self.n_units),
            nn.ReLU(),
        )
        self.out = nn.Linear(self.n_units, output_dim)

    def forward(self, x): # this defines the forward pass
        h = self.layers(x) # passes the input x through the hidden layers
        out = self.out(h) # passes the result of the hidden layers through the output layer
        return out*x*(1000-x)/(500**2)+(1000-x)*T_0/1000+x*T_env/1000
        #return out*(1-x/1000)*x/1000+T_0*(1-x/1000)+T_env*x/1000
    """ 
    The factor 1/500^2 is important as it gives all 3 terms the same importance. Otherwise far from 0 and 1000, the term out*x*(1000-x) would become way bigger than the other 2. 
    """

def predict(model: torch.nn.Module, X:np.array): # predict on the fitted model
        model.eval() # model.eval() tells your model that you are evaluating the model.
        out = model(np_to_th(X))
        return out.detach().numpy() 

                                       
# COMPUTE THE DERIVATIVE
def grad(outputs, inputs): # Computes the partial derivative of an output with respect to an input
    """
    Args:
        outputs: (N, 1) tensor
        inputs: (N, D) tensor
    """
    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True) 



# DEFINE THE LOSS FUNCTION FOR THE PINN WITH MEAN SQUARE ERROR
def physics_loss_mean(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    temps = model(evaluation_points)
    
    dT_dt = grad(temps, evaluation_points)[0]

    pde = R*(T_env - temps) - dT_dt
    
    return torch.mean(pde**2)
    # return torch.mean(pde**2)  # takes the mean of the square of the residue of the equation
#evaluation_points=np_to_th(np.linspace(0,10,10))
#print(physics_loss_mean(Net(input_dim=1,output_dim=1),evaluation_points=evaluation_points))




# DEFINE THE TRAINING PROCEDURE FOR THE UNIF-RAD
def fit_unif(model: torch.nn.Module, domain_points, learning_rate, epochs, number_of_eval_pts,resample_period): # this defines the training phase of the network
    
    domain_points_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    loss_total = []
    L2_error=[]
    Standard_dev=[]
    important_predictions=[]
    for ep in range(1,epochs+1):
        if ep==1 or ep % resample_period ==0: #CHANGING TOO OFTEN THE EVALUATION POINTS SLOWS DECREASES THE EFFICIENCY
            evaluation_points=np_to_th(np.random.uniform(low=domain_points[0],high=domain_points[-1],size=number_of_eval_pts))


        if ep==epochs or ep==11900:
                important_predictions.append(predict(model, domain_points))

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        Standard_dev.append(torch.std((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        loss_total.append(physics_loss_mean(model_2, domain_points_tensor).item()) # save the total loss value
        

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
        loss= physics_loss_mean(model, evaluation_points) # compute the loss on the evaluation points
        loss.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters of the model by subtracting the gradient 

        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs  """
    return loss_total, L2_error, Standard_dev, important_predictions



""" 
# RUN THE NEURAL NETWORK WITH UNIF-RAD
learning_rate=1e-5
number_of_eval_pts=40
epochs=10000
resample_period=1000
domain_points=np.linspace(0, 1000, 1000)

net_unif = Net(input_dim=1,output_dim=1)
start_unif = time.time()
losses,L2_error_unif,Standard_dev_unif,important_predictions_unif=fit_unif(model=net_unif, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_eval_pts=number_of_eval_pts,resample_period=resample_period)
#print(len(losses), len(L2_error_unif))
end_unif = time.time()

time_unif = round(end_unif - start_unif,1)

pinn_preds_unif = predict(net_unif, times)
difference_unif=np.abs(np.subtract(pinn_preds_unif.reshape(1,-1)[0],T_truth))

fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses), len(losses)),losses,c='orange',label='Error of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with random uniform sampling. \n  learning_rate={}, number_of_points={}, time={} s, resample_period={}'.format(learning_rate,number_of_eval_pts,time_unif,resample_period))

ax2=plt.subplot(gs[0,1])
plt.plot(times, T_truth, alpha=0.8,c='b',label='Analytical solution') # ground truth
plt.plot(times, pinn_preds_unif, alpha=0.8,c='orange',label='Uniform PINN') # prediction of the PINN neural network
plt.legend(labels=['Equation', 'PINN'])
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('Comparaison between the ground truth and the PINN with random uniform sampling. \n  learning_rate={}, number_of_points={}, time={} s, resample_period={}'.format(learning_rate,number_of_eval_pts,time_unif,resample_period))


ax3=plt.subplot(gs[1,1])
plt.plot(np.linspace(1, len(losses), len(losses)), L2_error_unif,c='k') # ground truth
plt.legend(labels=['L2 error'])
#plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('$L_2$ error between the PINN output and the ground truth. \n learning_rate={}, number_of_points={}, time={} s, resample_period={}'.format(learning_rate, number_of_eval_pts,time_unif,resample_period))


#ax3=plt.subplot(gs[1,1])
#plt.plot(times, difference_unif,c='k') # ground truth
#plt.legend(labels=['Difference to ground truth'])
#plt.ylabel('Temperature (°C)')
#plt.xlabel('Time (s)')
#plt.legend(loc="upper right")
#plt.title('Absolute difference between the PINN output and the ground truth. \n learning_rate={}, number_of_points={}, time={} s, resample_period={}'.format(learning_rate, number_of_eval_pts,time_unif,resample_period))

plt.subplots_adjust(hspace = .3)

plt.show()
 """




# DEFINE THE ZERO DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def physics_loss_zero(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    temps = model(evaluation_points)
    
    dT_dt = grad(temps, evaluation_points)[0]

    pde = (R*(T_env - temps) - dT_dt).detach().numpy().reshape(1,-1)[0]

    return pde**2
#print(physics_loss_zero(Net(input_dim=1,output_dim=1), np_to_th(np.linspace(0,1000,5))))





# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH RES-RAD
def fit_adapt_zero(model: torch.nn.Module, domain_points, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    loss_total = []
    L2_error=[]
    Standard_dev=[]
    important_predictions=[]
    for ep in range(1,epochs+1):

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            
        # COMPUTE THE COLLOCATION POINTS
        if ep==1 or ep % resample_period ==0: #CHANGING TOO OFTEN THE EVALUATION POINTS SLOWS DECREASES THE EFFICIENCY
            optimiser.zero_grad()
            total_points=np.random.uniform(low=domain_points[0],high=domain_points[-1], size=100*number_of_points)
            weights=np.abs(physics_loss_zero(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])

            collocation_points_tensor=np_to_th(collocation_points)

        if ep==epochs or ep==11900:
                important_predictions.append(predict(model, domain_points))

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        Standard_dev.append(torch.std((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        loss_total.append(physics_loss_mean(model_2, domain_points_tensor).item()) # save the total loss value

        optimiser.zero_grad()
        loss_integral = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_integral.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
       
        """
        if ep==epochs-1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs  """
    return loss_total, L2_error, Standard_dev, important_predictions


""" 
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH RES-RAD
learning_rate=1e-5
epochs=10000
number_of_points=40
kappa=1/2
c=0
resample_period=1000
domain_points=np.linspace(0, 1000, 1000)
net_zero = Net(input_dim=1,output_dim=1)

start_zero = time.time()
losses_adapt_zero, L2_error_zero, Standard_dev_zero, important_predictions_zero=fit_adapt_zero(model=net_zero, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_zero = time.time()

time_zero = round(end_zero - start_zero,1)

pinn_preds_zero = predict(net_zero, times)
difference_zero=np.abs(np.subtract(pinn_preds_zero.reshape(1,-1)[0],T_truth))

fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_adapt_zero), len(losses_adapt_zero)),losses_adapt_zero, c='orange',label='Error with adaptative sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.title('Loss of the PINN with adaptative sampling with $f$. \n  learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa,c,time_zero,resample_period))

ax2=plt.subplot(gs[0,1])
plt.plot(times, T_truth, alpha=0.8, c='b', label='Analytical solution') # ground truth
plt.plot(times, pinn_preds_zero, alpha=0.8, c='orange', label='Adaptative sampling') # prediction of the PINN neural network
plt.legend(labels=['Equation', 'PINN'])
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('Comparaison between the ground truth and the PINN output \n with adaptative sampling with $f$. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa, c, time_zero, resample_period))



ax3=plt.subplot(gs[1,1])
plt.plot(np.linspace(1, len(losses_adapt_zero),len(losses_adapt_zero)), L2_error_zero,c='k') # ground truth
plt.legend(labels=['L2 error'])
#plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('$L_2$ error between the PINN output with $f^\prime$ and the ground truth. \n learning_rate={}, number_of_points={}, time={} s, resample_period={}'.format(learning_rate, number_of_points,time_zero,resample_period))


#ax3=plt.subplot(gs[1,1])
#plt.plot(times, difference_prime,c='k') # ground truth
#plt.legend(labels=['Equation', 'PINN'])
#plt.ylabel('Temperature (°C)')
#plt.xlabel('Time (s)')
##plt.legend(loc="upper right")
#plt.title('Absolute difference between the PINN output with $f^\prime$ and the ground truth. \n learning_rate={}, #number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa, c, time_zero, resample_period))

plt.subplots_adjust(hspace = .3)
plt.show()
 """









# DEFINE THE DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    temps = model(evaluation_points)
    
    dT_dt = grad(temps, evaluation_points)[0]
    
    pde = R*(T_env - temps) - dT_dt
    
    return grad(pde**2,evaluation_points)[0].detach().numpy().reshape(1,-1)[0]

#print(deriv_physics_loss(Net(input_dim=1,output_dim=1), np_to_th(np.linspace(0,1000,5))))



# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH GRAD-RAD
def fit_adapt_prime(model: torch.nn.Module, domain_points, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    loss_total = []
    L2_error=[]
    Standard_dev=[]
    important_predictions=[]

    for ep in range(1,epochs+1):

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            
        # COMPUTE THE COLLOCATION POINTS
        if ep==1 or ep % resample_period ==0: #CHANGING TOO OFTEN THE EVALUATION POINTS SLOWS DECREASES THE EFFICIENCY
            optimiser.zero_grad()
            total_points=np.random.uniform(low=domain_points[0],high=domain_points[-1], size=100*number_of_points)
            weights=np.abs(deriv_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])

            collocation_points_tensor=np_to_th(collocation_points)

            
        if ep==epochs or ep==11900:
                important_predictions.append(predict(model, domain_points))

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        Standard_dev.append(torch.std((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        loss_total.append(physics_loss_mean(model_2, domain_points_tensor).item()) # save the total loss value

        optimiser.zero_grad()
        loss_integral = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_integral.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
       
        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs  """
    return loss_total, L2_error, Standard_dev, important_predictions

"""
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH GRAD-RAD
learning_rate=1e-5
epochs=10000
number_of_points=40
kappa=1/2
c=0
resample_period=1000
domain_points=np.linspace(0, 1000, 1000)
net_prime = Net(input_dim=1,output_dim=1)

start_prime = time.time()
losses_adapt_prime, L2_error_prime, Standard_dev_prime, important_predictions_prime=fit_adapt_prime(model=net_prime, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_prime = time.time()

time_prime = round(end_prime - start_prime,1)

print(len(important_predictions_prime))

pinn_preds_prime = predict(net_prime, times)
difference_prime=np.abs(np.subtract(pinn_preds_prime.reshape(1,-1)[0],T_truth))

fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_adapt_prime), len(losses_adapt_prime)),losses_adapt_prime, c='orange',label='Error with adaptative sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.title('Loss of the PINN with adaptative sampling with $f^\prime$. \n  learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa,c,time_prime,resample_period))

ax2=plt.subplot(gs[0,1])
plt.plot(times, T_truth, alpha=0.8, c='b', label='Analytical solution') # ground truth
plt.plot(times, pinn_preds_prime, alpha=0.8, c='orange', label='Adaptative sampling') # prediction of the PINN neural network
plt.legend(labels=['Equation', 'PINN'])
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('Comparaison between the ground truth and the PINN output \n with adaptative sampling and $f^\prime$. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa,c, time_prime, resample_period))



ax3=plt.subplot(gs[1,1])
plt.plot(np.linspace(1, len(losses_adapt_prime), len(losses_adapt_prime)), L2_error_prime,c='k') # ground truth
plt.legend(labels=['L2 error'])
#plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('$L_2$ error between the PINN output with $f^\prime$ and the ground truth. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa, c, time_prime, resample_period))


#ax3=plt.subplot(gs[1,1])
#plt.plot(times, difference_prime,c='k') # ground truth
#plt.legend(labels=['Equation', 'PINN'])
#plt.ylabel('Temperature (°C)')
#plt.xlabel('Time (s)')
##plt.legend(loc="upper right")
#plt.title('Absolute difference between the PINN output with $f^\prime$ and the ground truth. \n learning_rate={}, #number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa,c, time_prime, resample_period))

plt.subplots_adjust(hspace = .3)
plt.show()
 """



# DEFINE THE SECOND DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_sec_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    temps = model(evaluation_points)

    dT_dt = grad(temps, evaluation_points)[0]

    pde = R*(T_env - temps) - dT_dt
 
    return grad(grad(pde**2,evaluation_points)[0],evaluation_points)[0].detach().numpy().reshape(1,-1)[0]

#print(deriv_sec_physics_loss(Net(input_dim=1,output_dim=1), np_to_th(np.linspace(0,1000,100))))



# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH HESSIAN-RAD
def fit_adapt_second(model: torch.nn.Module, domain_points, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    loss_total = []
    L2_error=[]
    Standard_dev=[]
    important_predictions=[]
    for ep in range(1,epochs+1):
        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            
        # COMPUTE THE COLLOCATION POINTS
        if ep==1 or ep % resample_period ==0: #CHANGING TOO OFTEN THE EVALUATION POINTS SLOWS DECREASES THE EFFICIENCY
            total_points=np.random.uniform(low=domain_points[0],high=domain_points[-1], size=100*number_of_points)
            weights=np.abs(deriv_sec_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])

            collocation_points_tensor=np_to_th(collocation_points)

        if ep==epochs or ep==11900:
            important_predictions.append(predict(model, domain_points))

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        Standard_dev.append(torch.std((model_2(domain_points_tensor)-np_to_th(eq(domain_points)))**2).item())
        loss_total.append(physics_loss_mean(model_2, domain_points_tensor).item()) # save the total loss value

        optimiser.zero_grad()
        loss_integral = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_integral.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
       
        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs  """
    return loss_total, L2_error, Standard_dev, important_predictions


"""
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH HESSIAN-RAD
learning_rate=1e-5
epochs=1000
number_of_points=40
kappa=1/2
c=0
resample_period=1000
domain_points=np.linspace(0, 1000, 1000)
net_second = Net(input_dim=1,output_dim=1)

start_second = time.time()
losses_adapt_second, L2_error_second, Standard_dev_second, important_predictions_second=fit_adapt_second(model=net_second, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_second = time.time()

time_second = round(end_second - start_second,1)

#print(len(important_predictions_second))

pinn_preds_second = predict(net_second, times)
difference_second=np.abs(np.subtract(pinn_preds_second.reshape(1,-1)[0],T_truth))

fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_adapt_second), len(losses_adapt_second)),losses_adapt_second, c='orange',label='Error with adaptative sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.title('Loss of the PINN with adaptative sampling with $f^\prime$$^\prime$. \n  learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resample_period={}'.format(learning_rate, number_of_points, kappa, c, time_second, resample_period))

ax2=plt.subplot(gs[0,1])
plt.plot(times, T_truth, alpha=0.8, c='b', label='Analytical solution') # ground truth
plt.plot(times, pinn_preds_second, alpha=0.8, c='orange', label='Adaptative sampling') # prediction of the PINN neural network
plt.legend(labels=['Equation', 'PINN'])
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('Comparaison between the ground truth and the PINN output \n with adaptative sampling and $f^\prime$$^\prime$. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resample_period={}'.format(learning_rate, number_of_points, kappa, c, time_second, resample_period))


ax3=plt.subplot(gs[1,1])
plt.plot(np.linspace(1, len(losses_adapt_second), len(losses_adapt_second)), L2_error_second,c='k') # ground truth
plt.legend(labels=['L2 error'])
#plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
plt.title('$L_2$ error between the PINN output with $f^\prime$$^\prime$ and the ground truth. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resampling period={}'.format(learning_rate, number_of_points, kappa, c, time_second,resample_period))


#ax3=plt.subplot(gs[1,1])
#plt.plot(times, difference_second,c='k') # ground truth
#plt.legend(labels=['Equation', 'PINN'])
#plt.ylabel('Temperature (°C)')
#plt.xlabel('Time (s)')
#plt.legend(loc="upper right")
#plt.title('Absolute difference between the PINN output with $f^\prime$$^\prime$ and the ground truth. \n learning_rate={}, number_of_points={}, k={}, c={}, time={} s, resample_period={}'.format(learning_rate, number_of_points, kappa, c, time_second, resample_period))

plt.subplots_adjust(hspace = .3)
plt.show()

 """






# RUN THE THREE NEURAL NETWORKS
learning_rate=1e-5
epochs=30000
number_of_points=40
start_of_the_graph=0
kappa=1/2
c=0
resample_period=1000

domain_points=np.linspace(0, 1000, 1000)



start_unif = time.time()
net_unif = Net(input_dim=1,output_dim=1)
losses_unif, L2_error_unif, Standard_dev_unif, important_predictions_unif=fit_unif(model=net_unif, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_eval_pts=number_of_points, resample_period=resample_period)
end_unif = time.time()
time_unif = round(end_unif - start_unif,1)
pinn_preds_unif = predict(net_unif, times)
difference_unif=np.abs(np.subtract(pinn_preds_unif.reshape(1,-1)[0],T_truth))


start_zero = time.time()
net_adapt_zero = Net(input_dim=1,output_dim=1)
losses_zero,L2_error_zero,Standard_dev_zero,important_predictions_zero=fit_adapt_zero(model=net_adapt_zero, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_zero = time.time()
time_zero = round(end_zero - start_zero,1)
pinn_preds_adapt_zero = predict(net_adapt_zero, times)
difference_zero=np.abs(np.subtract(pinn_preds_adapt_zero.reshape(1,-1)[0],T_truth))


start_prime = time.time()
net_adapt_prime = Net(input_dim=1,output_dim=1)
losses_prime,L2_error_prime,Standard_dev_prime,important_predictions_prime=fit_adapt_prime(model=net_adapt_prime, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_prime = time.time()
time_prime = round(end_prime - start_prime,1)
pinn_preds_adapt_prime = predict(net_adapt_prime, times)
difference_prime=np.abs(np.subtract(pinn_preds_adapt_prime.reshape(1,-1)[0],T_truth))


start_second = time.time()
net_adapt_second = Net(input_dim=1,output_dim=1)
losses_second,L2_error_second,Standard_dev_second,important_predictions_second=fit_adapt_second(model=net_adapt_second, domain_points=domain_points, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_second = time.time()
time_second = round(end_second - start_second,1)
pinn_preds_adapt_second = predict(net_adapt_second, times)
difference_second=np.abs(np.subtract(pinn_preds_adapt_second.reshape(1,-1)[0],T_truth))

















#PLOT THE ERROR OF THE PINN FOR THE THREE METHODS AND THE PINN OUTPUTS

fig = plt.figure()
gs = gridspec.GridSpec(1, 3)

ax1=plt.subplot(gs[0,0])
plt.plot(times, difference_unif, alpha=0.8,c='b',label="unif-RAD") # prediction of the PINN neural network
plt.plot(times, difference_zero, alpha=0.8,c='orange',label="res-RAD")  # prediction of the PINN neural 
plt.plot(times, difference_prime, alpha=0.8,c='r',label="grad-RAD")  # prediction of the PINN neural network
plt.plot(times, difference_second, alpha=0.8,c='g',label="hessian-RAD")  # prediction of the PINN neural network
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
ax1.set_box_aspect(1)
plt.title('Absolute differences between the ground truth \n and the PINN outputs \n after training {} epochs'.format(epochs))

ax2=plt.subplot(gs[0,1])
plt.plot(times, np.abs(np.subtract(important_predictions_unif[1].reshape(1,-1)[0],T_truth)), alpha=0.8,c='b',label="unif-RAD")  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_zero[1].reshape(1,-1)[0],T_truth)), alpha=0.8,c='orange',label="res-RAD")  # prediction of the PINN neural 
plt.plot(times, np.abs(np.subtract(important_predictions_prime[1].reshape(1,-1)[0],T_truth)), alpha=0.8,c='r',label="grad-RAD")  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_second[1].reshape(1,-1)[0],T_truth)), alpha=0.8,c='g',label="hessian-RAD")  # prediction of the PINN neural network
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
ax2.set_box_aspect(1)
plt.title('Absolute differences between the ground truth \n  and the PINN outputs \n before the last update ({} epochs)'.format(epochs))

ax3=plt.subplot(gs[0,2])
plt.plot(times, np.abs(np.subtract(important_predictions_unif[0].reshape(1,-1)[0],T_truth)), alpha=0.8,c='b',label="unif-RAD")  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_zero[0].reshape(1,-1)[0],T_truth)), alpha=0.8,c='orange',label="res-RAD")  # prediction of the PINN neural 
plt.plot(times, np.abs(np.subtract(important_predictions_prime[0].reshape(1,-1)[0],T_truth)), alpha=0.8,c='r',label="grad-RAD")  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_second[0].reshape(1,-1)[0],T_truth)), alpha=0.8,c='g',label="hessian-RAD")  # prediction of the PINN neural network
plt.ylabel('Temperature (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right")
ax3.set_box_aspect(1)
plt.title('Absolute differences between the ground truth \n and the PINN outputs after 11900 epochs') 

#plt.subplots_adjust(hspace = .5)

fig.suptitle('Settings: learning_rate={}, number_of_points={}, k={}, c={}, resample_period={}'.format(learning_rate, number_of_points, kappa, c,  resample_period), fontsize=20, y=0.85)
plt.show()  




fig = plt.figure()
gs = gridspec.GridSpec(1, 2)
ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_unif), len(L2_error_unif))[0::100], L2_error_unif[0::100], alpha=0.8,c='b',label='unif-RAD, t={} s'.format(time_unif)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_zero), len(L2_error_zero))[0::100], L2_error_zero[0::100], alpha=0.8,c='orange',label="res-RAD, t={} s".format(time_zero)) # prediction of the PINN neural network
#plt.plot(np.append(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[29900:],30001), L2_error_prime[29900:]+[np.mean(difference_prime)], alpha=0.8,c='r',label="Adaptative sampling with $f^\prime$") # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[0::100], L2_error_prime[0::100], alpha=0.8,c='r',label="grad-RAD, t={} s".format(time_prime)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_second), len(L2_error_second))[0::100], L2_error_second[0::100], alpha=0.8,c='g',label="hessian-RAD, t={} s".format(time_second)) # prediction of the PINN neural network
#plt.ylabel('Temperature (°C)')
plt.xlabel('Epochs')
plt.legend(loc="upper right", prop={'size': 20})
plt.title('Mean of the $L_2$ between the ground truth \n and the PINN outputs during training')

ax2=plt.subplot(gs[0,1])
plt.plot(np.linspace(1, len(Standard_dev_unif), len(Standard_dev_unif))[0::100], Standard_dev_unif[0::100], alpha=0.8,c='b',label='unif-RAD, t={} s'.format(time_unif)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(Standard_dev_zero), len(Standard_dev_zero))[0::100], Standard_dev_zero[0::100], alpha=0.8,c='orange',label="res-RAD, t={} s".format(time_zero)) # prediction of the PINN neural network
#plt.plot(np.append(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[29900:],30001), L2_error_prime[29900:]+[np.mean(difference_prime)], alpha=0.8,c='r',label="Adaptative sampling with $f^\prime$") # prediction of the PINN neural network
plt.plot(np.linspace(1, len(Standard_dev_prime), len(Standard_dev_prime))[0::100], Standard_dev_prime[0::100], alpha=0.8,c='r',label="grad-RAD, t={} s".format(time_prime)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(Standard_dev_second), len(Standard_dev_second))[0::100], Standard_dev_second[0::100], alpha=0.8,c='g',label="hessian-RAD, t={} s".format(time_second)) # prediction of the PINN neural network
#plt.ylabel('Temperature (°C)')
plt.xlabel('Epochs')
plt.legend(loc="upper right", prop={'size': 20})
plt.title('Standard deviation of the $L_2$ between the ground truth \n and the PINN outputs during training')
plt.show()



fig,ax=plt.subplots()
plt.plot(times, np.abs(np.subtract(important_predictions_unif[0].reshape(1,-1)[0],T_truth)**2), alpha=0.8,c='b',label="unif-RAD, mean $L_2$={}".format(round(L2_error_unif[11899],1)))  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_zero[0].reshape(1,-1)[0],T_truth)**2), alpha=0.8,c='orange',label="res-RAD, mean $L_2$={}".format(round(L2_error_zero[11899],1)))  # prediction of the PINN neural 
plt.plot(times, np.abs(np.subtract(important_predictions_prime[0].reshape(1,-1)[0],T_truth)**2), alpha=0.8,c='r',label="grad-RAD, mean $L_2$={}".format(round(L2_error_prime[11899],1)))  # prediction of the PINN neural network
plt.plot(times, np.abs(np.subtract(important_predictions_second[0].reshape(1,-1)[0],T_truth)**2), alpha=0.8,c='g',label="hessian-RAD, mean $L_2$={}".format(round(L2_error_second[11899],1)))  # prediction of the PINN neural network
#plt.ylabel('Absolute error (°C)')
plt.xlabel('Time (s)')
plt.legend(loc="upper right", prop={'size': 25})
plt.title('Square of the absolute differences between the ground truth and the PINN outputs after 11900 epochs') 
plt.show() 



fig,ax=plt.subplots()
plt.plot(np.linspace(1, len(L2_error_unif), len(L2_error_unif))[100:12000][0::100], L2_error_unif[100:12000][0::100], alpha=0.8,c='b',label='unif-RAD, t={} s'.format(time_unif)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_zero), len(L2_error_zero))[100:12000][0::100], L2_error_zero[100:12000][0::100], alpha=0.8,c='orange',label='res-RAD, t={} s'.format(time_zero)) # prediction of the PINN neural network
#plt.plot(np.append(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[29900:],30001), L2_error_prime[29900:]+[np.mean(difference_prime)], alpha=0.8,c='r',label="Adaptative sampling with $f^\prime$") # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[100:12000][0::100], L2_error_prime[100:12000][0::100], alpha=0.8,c='r',label='grad-RAD, t={} s'.format(time_prime)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_second), len(L2_error_second))[100:12000][0::100], L2_error_second[100:12000][0::100], alpha=0.8,c='g',label='hessian-RAD, t={} s'.format(time_second)) # prediction of the PINN neural network
#plt.ylabel('Temperature (°C)')
plt.xlabel('Epochs')
#leg1 = ax.legend(['t={} s'.format(time_unif),'t={} s'.format(time_zero), 't={} s'.format(time_prime), 't={} s'.format(time_second)], loc="lower left", prop={'size': 25})
#ax.add_artist(leg1)
plt.legend(loc="upper right", prop={'size': 25})
plt.title('$L_2$-error between the ground truth \n and the PINN outputs during training')
plt.show() 

