# THE EQUATION IS -\Delta(u)=f WITH u(x,y) AS THE FUNCTION


import torch
import torch.nn as nn
import torch.optim as optim
import functools
import matplotlib as mpl ; mpl.rcParams['text.usetex'] = True # to use latex rendering in the legends and titles
import matplotlib.pyplot as plt
import numpy as np
import torch
import math
from numpy import sqrt, sin, cos, pi
import matplotlib.gridspec as gridspec
import time
import copy
import pickle
from mpl_toolkits.axes_grid1 import make_axes_locatable

torch.manual_seed(42)
np.random.seed(10)






# DEFINE THE GROUND TRUTH FUNCTION
a=10
def analytical_solution(Z:np.array):
    return (16*Z[0]*(1-Z[0])*Z[1]*(1-Z[1]))**a

def analytical_solution_x_y(x:np.array,y:np.array):
    return (16*x*(1-x)*y*(1-y))**a


""" 
#PLOT THE ANALYTICAL SOLUTION
x_domain=np.linspace(0, 1, 1000)
y_domain=np.linspace(0, 1, 1000)
analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

fig = plt.figure()
ax=plt.subplot()
im=ax.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis')
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im, ax=ax) 
plt.show()
"""


def f(z:torch.Tensor):
    u_xx=a*16**a*z[:,1]**a*(1 - z[:,1])**a*(z[:,0]**(a - 2)*(1 - z[:,0])**a*a - 2*z[:,0]**(a - 1)*(1 - z[:,0])**(a - 1)*a + z[:,0]**a*(1 - z[:,0])**(a - 2)*a - z[:,0]**(a - 2)*(1 - z[:,0])**a - z[:,0]**a*(1 - z[:,0])**(a - 2))

    u_yy=a*16**a*(1 - z[:,0])**a*z[:,0]**a*(z[:,1]**a*(1 - z[:,1])**(a - 2)*a + (1 - z[:,1])**a*z[:,1]**(a - 2)*a - 2*z[:,1]**(a - 1)*(1 - z[:,1])**(a - 1)*a - z[:,1]**a*(1 - z[:,1])**(a - 2) - (1 - z[:,1])**a*z[:,1]**(a - 2))
   
    return -u_xx-u_yy

# CREATE A FUNCTION TO TURN TWO LISTS INTO A GRID
def make_grid(x:np.array, y:np.array):
    return np.array(np.meshgrid(x, y)).T.reshape(-1,2)


# CREATE A FUNCTION TO TURN A 2D LIST INTO TORCH TENSOR
def np_to_th(z:np.array):
    n_samples = z.shape[0]
    return torch.from_numpy(z).to(torch.float).reshape(n_samples, -1).requires_grad_(True)

#x_domain=np.linspace(0.1, 0.9, 2)
#y_domain=np.linspace(0.1, 0.5, 2)
#b=make_grid(x_domain,y_domain)
#print(b)
#print(np.array([analytical_solution(Z) for Z in b]))
#input=np_to_th(make_grid(x_domain,y_domain))
#print(input[:,0].size())
#print(f(input).size())



# CONSTRUCT THE NEURAL NETWORK
class Net(nn.Module):
    def __init__(
        self,
        input_dim, # dimension of the input layer
        output_dim, # dimension of the output layer
        n_units=20, # number of neurones in each hidden layer
    ) -> None: #  ?????????????????????????????????????????????????????????????????????????????????????????? 
        super().__init__() #  ??????????????????????????????????????????????????????????????????????????????
        self.n_units = n_units
        self.layers = nn.Sequential(
            nn.Linear(input_dim, self.n_units),
            nn.Tanh(),                                # we use tanh as we will need second derivatives of the network output, and the second derivative of ReLU is zero
            nn.Linear(self.n_units, self.n_units),
            nn.Tanh(),
            nn.Linear(self.n_units, self.n_units),
            nn.Tanh(),
            #nn.Linear(self.n_units, self.n_units),
            #nn.Tanh(),
        )
        self.out = nn.Linear(self.n_units, output_dim)

    def forward(self, input): # this defines the forward pass      input=(x,t)
        h = self.layers(input) # passes the inputs x and t through the hidden layers
        out = self.out(h) # passes the result of the hidden layers through the output layer
        
        return (input[:,0]*(1-input[:,0])*input[:,1]*(1-input[:,1])*out.squeeze()).reshape(-1,1)
        #return input[:,0].reshape(-1,1)*(1-input[:,0].reshape(-1,1))*input[:,1].reshape(-1,1)*(1-input[:,1].reshape(-1,1))*out
#x_domain=np.linspace(0, 1, 5)
#y_domain=np.linspace(0, 1, 5)
#input=np_to_th(make_grid(x_domain,y_domain))
#print(input[:,0].reshape(-1,1).size())
#print(Net(input_dim=2,output_dim=1)(np_to_th(make_grid(x_domain,y_domain))).size())

def predict(model, x,y): # predict on the fitted model
        model.eval() # model.eval() tells your model that you are evaluating the model.
        out = model(np_to_th(make_grid(x,y)))
        return out.detach().numpy() 

# COMPUTE THE DERIVATIVE
def grad(outputs: torch.Tensor, inputs: torch.Tensor): # Computes the partial derivative of an output with respect to an input
    """
    Args:
        outputs: (N, 1) tensor
        inputs: (N, D) tensor
    """
    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True) 


# DEFINE THE LOSS FUNCTION FOR THE PINN WITH MEAN SQUARE ERROR
def physics_loss_mean(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    d2u_dy2=grad(grad(u, evaluation_points)[0][:,1],evaluation_points)[0][:,1]
    
    PDE = d2u_dx2+d2u_dy2+f(evaluation_points)
    
    return torch.mean(PDE**2)  # takes the mean of the square of the residue of the equation
#x_domain=np.linspace(0, 1, 2)
#y_domain=np.linspace(0, 1, 5)
#print(physics_loss_mean(Net(input_dim=2,output_dim=1), np_to_th(make_grid(x_domain,y_domain))))
   


# DEFINE THE TRAINING PROCEDURE FOR THE GRID-RAD
def fit_grid(model: torch.nn.Module, x_domain, y_domain, number_of_points, learning_rate, epochs): # this defines the training phase of the network
    domain_points=make_grid(x_domain,y_domain)
    domain_tensor=np_to_th(domain_points)
    evaluation_points=make_grid(np.linspace(x_domain[0],x_domain[-1],np.sqrt(number_of_points).astype(int)),np.linspace(y_domain[0],y_domain[-1],np.sqrt(number_of_points).astype(int)))
    evaluation_points_tensor=np_to_th(evaluation_points)

    model_2=copy.deepcopy(model)


    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
    
    losses_total = []
    L2_error=[]
    Standard_dev=[]
    for ep in range(1,epochs+1):
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        Standard_dev.append(torch.std((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
        loss = physics_loss_mean(model, evaluation_points_tensor) # compute the loss on the evaluation points
        loss.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters of the model by subtracting the gradient 

        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs
        """
    return losses_total, evaluation_points, L2_error, Standard_dev




"""
# RUN THE NEURAL NETWORK WITH GRID-RAD
learning_rate=1e-3
epochs=20000
number_of_points=400
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)

net_grid = Net(input_dim=2,output_dim=1)
start_grid = time.time()
losses_grid,eval_points_grid,L2_error_grid,Standard_dev_grid=fit_grid(model=net_grid, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points,learning_rate=learning_rate, epochs=epochs)
end_grid = time.time()
time_grid = round(end_grid - start_grid,1)
#print("It took", time_grid, "seconds")

pinn_preds_grid = predict(net_grid, x_domain,y_domain)

pinn_preds_grid_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_grid_matrix[i][j]=(pinn_preds_grid[i*(len(y_domain))+j].item())

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

difference_matrix_grid=np.abs(analytical_pred_matrix-pinn_preds_grid_matrix)

max_value_diff_grid=np.max([np.max(l) for l in difference_matrix_grid])
min_value_diff_grid=np.min([np.min(l) for l in difference_matrix_grid])

# PLOT THE LOSS AND THE OUTPUT OF THE NETWORK TO COMPARE WITH THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_grid), len(losses_grid)),losses_grid,c='orange',label='Loss of the PINN with grid-RAD') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with grid-RAD. t={} s. \n  learning_rate={}, nb points={}'.format(time_grid,epochs,learning_rate,number_of_points))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im2, ax=ax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_grid_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of the PINN after {} epochs with grid-RAD. \n  learning_rate={}, nb points={}'.format(epochs,learning_rate,number_of_points))
fig.colorbar(im3, ax=ax3) 

plt.subplots_adjust(hspace = .3)
plt.figure()




# PLOT THE L2 ERROR AND THE ABSOLUTE DIFFERENCE BETWEEN THE OUTPUT OF THE NETWORK AND THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(1, 2)

ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_grid), len(L2_error_grid)),L2_error_grid,c='orange',label='$L_2$-error of the PINN with grid-RAD') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('$L_2$-error of the PINN with grid-RAD. \n  nb points={}'.format(epochs,learning_rate,number_of_points))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(difference_matrix_grid.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='plasma', vmin = min_value_diff_grid, vmax = max_value_diff_grid)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error between the grid-RAD PINN and the analytical solution after {} epochs. \n learning_rate={}, nb points={}'.format(epochs, learning_rate, number_of_points))
plt.colorbar(im2, ax=ax2) 
plt.figure()

plt.show()

# PLOT THE EVALUATION POINTS
x_eval=[z[0] for z in eval_points_grid]
y_eval=[z[1] for z in eval_points_grid]
plt.scatter(x_eval,y_eval)
plt.xlabel('t')
plt.ylabel('x')
plt.title('Last sample points used for the training phase. \n nb conformal points ={}'.format(number_of_points))
plt.show()
"""



# DEFINE THE TRAINING PROCEDURE FOR THE RANDOM UNIFORM METHOD
def fit_unif(model: torch.nn.Module, x_domain, y_domain, number_of_points, learning_rate, epochs, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,y_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)


    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
    
    losses_total = []
    L2_error=[]
    Standard_dev=[]
    for ep in range(1,epochs+1):
        if ep==1 or ep % resample_period ==0:
            evaluation_points = np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(number_of_points,2)) # we selected randomly and uniformely number_of_points in the domain
            evaluation_points_tensor=np_to_th(evaluation_points)

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        Standard_dev.append(torch.std((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
        loss = physics_loss_mean(model, evaluation_points_tensor) # compute the loss on the evaluation points
        loss.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters of the model by subtracting the gradient 

        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses[-1]:.8f}") # print the loss every epochs/10 epochs
        """
    return losses_total, evaluation_points, L2_error, Standard_dev




""" 
# RUN THE NEURAL NETWORK WITH UNIFORM SAMPLING
learning_rate=1e-3
epochs=20000
number_of_points=400
resample_period=1000
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)

net_unif = Net(input_dim=2,output_dim=1)
start_unif = time.time()
losses_unif,eval_points_unif,L2_error_unif,Standard_dev_unif=fit_unif(model=net_unif, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points,learning_rate=learning_rate, epochs=epochs, resample_period=resample_period)
end_unif = time.time()
time_unif = round(end_unif - start_unif,1)
#print("It took", time_unif, "seconds")

pinn_preds_unif = predict(net_unif, x_domain,y_domain)

pinn_preds_unif_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_unif_matrix[i][j]=(pinn_preds_unif[i*(len(y_domain))+j].item())

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

difference_matrix_unif=np.abs(analytical_pred_matrix-pinn_preds_unif_matrix)

max_value_diff_unif=np.max([np.max(l) for l in difference_matrix_unif])
min_value_diff_unif=np.min([np.min(l) for l in difference_matrix_unif])

# PLOT THE LOSS AND THE OUTPUT OF THE NETWORK TO COMPARE WITH THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_unif), len(losses_unif)),losses_unif,c='orange',label='Loss of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with random uniform sampling. t={} s. \n  learning_rate={}, nb points={}, resample_period={}'.format(time_unif,epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im2, ax=ax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_unif_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of the PINN after {} epochs with random uniform sampling. \n  learning_rate={}, nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))
fig.colorbar(im3, ax=ax3) 

plt.subplots_adjust(hspace = .3)
plt.figure()




# PLOT THE L2 ERROR AND THE ABSOLUTE DIFFERENCE BETWEEN THE OUTPUT OF THE NETWORK AND THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(1, 2)

ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_unif), len(L2_error_unif)),L2_error_unif,c='orange',label='$L_2$-error of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('$L_2$-error of the PINN with random uniform sampling. \n  nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(difference_matrix_unif.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='plasma', vmin = min_value_diff_unif, vmax = max_value_diff_unif)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error between the uniform PINN and the analytical solution after {} epochs. \n learning_rate={}, nb points={}, resample_period={}'.format(epochs, learning_rate, number_of_points, resample_period))
plt.colorbar(im2, ax=ax2) 
plt.figure()

plt.show()

# PLOT THE EVALUATION POINTS
x_eval=[z[0] for z in eval_points_unif]
y_eval=[z[1] for z in eval_points_unif]
plt.scatter(x_eval,y_eval)
plt.xlabel('t')
plt.ylabel('x')
plt.title('Last sample points used for the training phase. \n nb conformal points ={}'.format(number_of_points))
plt.show()
"""








# DEFINE THE ZERO DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def physics_loss_zero(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    d2u_dy2=grad(grad(u, evaluation_points)[0][:,1],evaluation_points)[0][:,1]
    
    PDE = d2u_dx2+d2u_dy2+f(evaluation_points)

    return (PDE**2).detach().numpy()
#x_domain=np.linspace(0, 1, 2)
#y_domain=np.linspace(-2, 2, 5)
#print(physics_loss_zero(Net(input_dim=2,output_dim=1), np_to_th(make_grid(x_domain,y_domain))))


# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE ZERO DERIVATIVE
def fit_adapt_zero(model: torch.nn.Module, x_domain, y_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,y_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    Standard_dev=[]
    stored_collocations_points=[]

    
    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(physics_loss_zero(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        Standard_dev.append(torch.std((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses_total[-1]:.8f}") # print the loss every epochs/10 epochs  
        """
    return losses_total, stored_collocations_points, L2_error, Standard_dev



"""
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH THE ZERO DERIVATIVE
learning_rate=1e-3
epochs=1001
number_of_points=400
resample_period=1000
kappa=1/2
c=0
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)


net_zero = Net(input_dim=2,output_dim=1)
start_zero = time.time()
losses_zero,stored_collocations_points_zero,L2_error_zero,Standard_dev_zero=fit_adapt_zero(model=net_zero, x_domain=x_domain, y_domain=y_domain, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_zero = time.time()
time_zero = round(end_zero - start_zero,1)

pinn_preds_zero = predict(net_zero, x_domain,y_domain)

pinn_preds_zero_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_zero_matrix[i][j]=(pinn_preds_zero[i*(len(y_domain))+j].item())

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

difference_matrix_zero=np.abs(analytical_pred_matrix-pinn_preds_zero_matrix)

max_value_diff_zero=np.max([np.max(l) for l in difference_matrix_zero])
min_value_diff_zero=np.min([np.min(l) for l in difference_matrix_zero])


# PLOT THE LOSS AND THE OUTPUT OF THE NETWORK WITH ADAPTATIVE SAMPLING WITH THE ZERO DERIVATIVE TO COMPARE WITH THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_zero), len(losses_zero)),losses_zero,c='orange',label='Loss of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with adaptative sampling with $f^\prime$. t={} s. \n  learning_rate={}, nb points={}, resample_period={}'.format(time_zero,epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im2, ax=ax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_zero_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of the PINN after {} epochs with adaptative sampling with $f^\prime$. \n  learning_rate={}, nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))
fig.colorbar(im3, ax=ax3) 

plt.subplots_adjust(hspace = .3)
plt.figure()




# PLOT THE L2 ERROR AND THE ABSOLUTE DIFFERENCE BETWEEN THE OUTPUT OF THE NETWORK AND THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(1, 2)

ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_zero), len(L2_error_zero)),L2_error_zero,c='orange',label='$L_2$-error of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('$L_2$-error of the PINN with $f^\prime$. \n  nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(difference_matrix_zero.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='plasma', vmin = min_value_diff_zero, vmax = max_value_diff_zero)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error between the PINN with $f^\prime$ and the analytical solution after {} epochs. \n learning_rate={}, nb points={}, resample_period={}'.format(epochs, learning_rate, number_of_points, resample_period))
plt.colorbar(im2, ax=ax2) 
plt.figure()

plt.show()

 



# PLOT THE COLLOCATIONS POINTS AT REGULAR INTERVALS
parameters=[i for i in range(len(stored_collocations_points_zero))] # the range of the number of points we want to consider
number_of_column=4 # the number of columns in the plot
figure, axis = plt.subplots(math.ceil(len(parameters)/number_of_column),number_of_column)
figure.tight_layout()
grid_indices=[1+i for i in range(math.ceil(len(parameters)/number_of_column)*number_of_column)]
#print(grid_indices)
#print(parameters)
#print(axis.flatten())

for par, ax in zip(parameters, axis.flatten()[:len(parameters)]):
    ax.scatter(stored_collocations_points_zero[par][:,1],stored_collocations_points_zero[par][:,0], c='k',marker="o")
    if par==0:
        ax.set_title("Epoch=1")
    else:
        ax.set_title("Epoch={}".format(par*resample_period))
    
for i in range(len(grid_indices)-len(parameters)):
    axis.flatten()[-1-i].set_visible(False) 
figure.subplots_adjust(top=0.83)
figure.suptitle("Plots of the collocation points at each resampling with $f^\prime$. \n learning_rate={}, number_of_points={}".format(learning_rate,number_of_points), fontsize=20)
plt.show()

 """




# DEFINE THE NORM OF THE DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): 
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    d2u_dy2=grad(grad(u, evaluation_points)[0][:,1],evaluation_points)[0][:,1]
    
    PDE = d2u_dx2+d2u_dy2+f(evaluation_points)
    
    gradient_x=grad(torch.mean(PDE**2),evaluation_points)[0][:,0]
    gradient_y=grad(torch.mean(PDE**2),evaluation_points)[0][:,1]

    return torch.sqrt(gradient_x**2+gradient_y**2).detach().numpy()
    
#t_domain=np.linspace(0, 1, 2)
#x_domain=np.linspace(-2, 2, 5)
#print(deriv_physics_loss(Net(input_dim=2,output_dim=1), np_to_th(make_grid(x_domain,t_domain))))




# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE FIRST DERIVATIVE
def fit_adapt_prime(model: torch.nn.Module, x_domain, y_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,y_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    Standard_dev=[]
    stored_collocations_points=[]

    
    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(deriv_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        Standard_dev.append(torch.std((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses_total[-1]:.8f}") # print the loss every epochs/10 epochs  
        """
    return losses_total, stored_collocations_points, L2_error, Standard_dev
    
""" 
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH THE FIRST DERIVATIVE
learning_rate=1e-3
epochs=20000
number_of_points=400
resample_period=1000
kappa=1/2
c=0
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)


net_prime = Net(input_dim=2,output_dim=1)
start_prime = time.time()
losses_prime,stored_collocations_points_prime,L2_error_prime,Standard_dev_prime=fit_adapt_prime(model=net_prime, x_domain=x_domain, y_domain=y_domain, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_prime = time.time()
time_prime = round(end_prime - start_prime,1)

pinn_preds_prime = predict(net_prime, x_domain,y_domain)

pinn_preds_prime_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_prime_matrix[i][j]=(pinn_preds_prime[i*(len(y_domain))+j].item())

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

difference_matrix_prime=np.abs(analytical_pred_matrix-pinn_preds_prime_matrix)

max_value_diff_prime=np.max([np.max(l) for l in difference_matrix_prime])
min_value_diff_prime=np.min([np.min(l) for l in difference_matrix_prime])


# PLOT THE LOSS AND THE OUTPUT OF THE NETWORK WITH ADAPTATIVE SAMPLING WITH THE FIRST DERIVATIVE TO COMPARE WITH THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_prime), len(losses_prime)),losses_prime,c='orange',label='Loss of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with adaptative sampling with $f^\prime$. t={} s. \n  learning_rate={}, nb points={}, resample_period={}'.format(time_prime,epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im2, ax=ax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_prime_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of the PINN after {} epochs with adaptative sampling with $f^\prime$. \n  learning_rate={}, nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))
fig.colorbar(im3, ax=ax3) 

plt.subplots_adjust(hspace = .3)
plt.figure()




# PLOT THE L2 ERROR AND THE ABSOLUTE DIFFERENCE BETWEEN THE OUTPUT OF THE NETWORK AND THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(1, 2)

ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_prime), len(L2_error_prime)),L2_error_prime,c='orange',label='$L_2$-error of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('$L_2$-error of the PINN with $f^\prime$. \n  nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(difference_matrix_prime.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='plasma', vmin = min_value_diff_prime, vmax = max_value_diff_prime)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error between the PINN with $f^\prime$ and the analytical solution after {} epochs. \n learning_rate={}, nb points={}, resample_period={}'.format(epochs, learning_rate, number_of_points, resample_period))
plt.colorbar(im2, ax=ax2) 
plt.figure()

plt.show()

 



# PLOT THE COLLOCATIONS POINTS AT REGULAR INTERVALS
parameters=[i for i in range(len(stored_collocations_points_prime))] # the range of the number of points we want to consider
number_of_column=4 # the number of columns in the plot
figure, axis = plt.subplots(math.ceil(len(parameters)/number_of_column),number_of_column)
figure.tight_layout()
grid_indices=[1+i for i in range(math.ceil(len(parameters)/number_of_column)*number_of_column)]
#print(grid_indices)
#print(parameters)
#print(axis.flatten())

for par, ax in zip(parameters, axis.flatten()[:len(parameters)]):
    ax.scatter(stored_collocations_points_prime[par][:,1],stored_collocations_points_prime[par][:,0], c='k',marker="o")
    if par==0:
        ax.set_title("Epoch=1")
    else:
        ax.set_title("Epoch={}".format(par*resample_period))
    
for i in range(len(grid_indices)-len(parameters)):
    axis.flatten()[-1-i].set_visible(False) 
figure.subplots_adjust(top=0.83)
figure.suptitle("Plots of the collocation points at each resampling with $f^\prime$. \n learning_rate={}, number_of_points={}".format(learning_rate,number_of_points), fontsize=20)
plt.show()

 """





# DEFINE THE NORM OF THE SECOND DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_sec_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): 
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    d2u_dy2=grad(grad(u, evaluation_points)[0][:,1],evaluation_points)[0][:,1]
    
    PDE = d2u_dx2+d2u_dy2+f(evaluation_points)

    dPDE2_dx=grad(torch.mean(PDE**2),evaluation_points)[0][:,0]
    dPDE2_dy=grad(torch.mean(PDE**2),evaluation_points)[0][:,1]
    d2PDE2_dx2=grad(dPDE2_dx,evaluation_points)[0][:,0]
    d2PDE2_dxdy=grad(dPDE2_dx,evaluation_points)[0][:,1]
    d2PDE2_dydx=grad(dPDE2_dy,evaluation_points)[0][:,0]
    d2PDE2_dy2=grad(dPDE2_dy,evaluation_points)[0][:,1]

    return torch.sqrt(d2PDE2_dx2**2+d2PDE2_dy2**2+d2PDE2_dxdy**2+d2PDE2_dydx**2).detach().numpy()
    
#t_domain=np.linspace(0, 1, 2)
#x_domain=np.linspace(-2, 2, 5)
#print(np_to_th(make_grid(x_domain,t_domain)).size())
#print(deriv_sec_physics_loss(Net(input_dim=2,output_dim=1), np_to_th(make_grid(x_domain,t_domain))))



# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE SECOND DERIVATIVE
def fit_adapt_second(model: torch.nn.Module, x_domain, y_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,y_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    Standard_dev=[]
    stored_collocations_points=[]

    
    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],y_domain[0]], high=[x_domain[-1],y_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(deriv_sec_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        Standard_dev.append(torch.std((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        """ 
        if ep % int(epochs / 10) == 0 or ep==1:
            print(f"Epoch {ep}/{epochs}, loss: {losses_total[-1]:.8f}") # print the loss every epochs/10 epochs  
        """
    return losses_total, stored_collocations_points, L2_error, Standard_dev
    
"""
# RUN THE NEURAL NETWORK WITH THE ADAPTATIVE SAMPLING WITH THE SECOND DERIVATIVE
learning_rate=1e-3
epochs=20000
number_of_points=400
resample_period=1000
kappa=1/2
c=0
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)


net_second = Net(input_dim=2,output_dim=1)
start_second = time.time()
losses_second,stored_collocations_points_second,L2_error_second,Standard_dev_second=fit_adapt_second(model=net_second, x_domain=x_domain, y_domain=y_domain, learning_rate=learning_rate, epochs=epochs, number_of_points=number_of_points, kappa=kappa, c=c, resample_period=resample_period)
end_second = time.time()
time_second = round(end_second - start_second,1)

pinn_preds_second = predict(net_second, x_domain,y_domain)

pinn_preds_second_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_second_matrix[i][j]=(pinn_preds_second[i*(len(y_domain))+j].item())

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))

difference_matrix_second=np.abs(analytical_pred_matrix-pinn_preds_second_matrix)

max_value_diff_second=np.max([np.max(l) for l in difference_matrix_second])
min_value_diff_second=np.min([np.min(l) for l in difference_matrix_second])


# PLOT THE LOSS AND THE OUTPUT OF THE NETWORK WITH ADAPTATIVE SAMPLING WITH THE FIRST DERIVATIVE TO COMPARE WITH THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[:,0])
plt.plot(np.linspace(1, len(losses_second), len(losses_second)),losses_second,c='orange',label='Loss of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('Loss of the PINN with adaptative sampling with $f^\prime$$^\prime$. t={} s. \n  learning_rate={}, nb points={}, resample_period={}'.format(time_second,epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of the analytical solution')
fig.colorbar(im2, ax=ax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_second_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='viridis')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of the PINN after {} epochs with adaptative sampling with $f^\prime$$^\prime$. \n  learning_rate={}, nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))
fig.colorbar(im3, ax=ax3) 

plt.subplots_adjust(hspace = .3)
plt.figure()




# PLOT THE L2 ERROR AND THE ABSOLUTE DIFFERENCE BETWEEN THE OUTPUT OF THE NETWORK AND THE ANALYTICAL SOLUTION
fig = plt.figure()
gs = gridspec.GridSpec(1, 2)

ax1=plt.subplot(gs[0,0])
plt.plot(np.linspace(1, len(L2_error_second), len(L2_error_second)),L2_error_second,c='orange',label='$L_2$-error of the PINN with uniform sampling') # plot the loss evolution of the PINN
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend(loc="upper right")
plt.title('$L_2$-error of the PINN with $f^\prime$$^\prime$. \n  nb points={}, resample_period={}'.format(epochs,learning_rate,number_of_points, resample_period))

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(difference_matrix_second.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='auto',cmap='plasma', vmin = min_value_diff_second, vmax = max_value_diff_second)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error between the PINN with $f^\prime$$^\prime$ and the analytical solution after {} epochs. \n learning_rate={}, nb points={}, resample_period={}'.format(epochs, learning_rate, number_of_points, resample_period))
plt.colorbar(im2, ax=ax2) 
plt.figure()

plt.show()

 



# PLOT THE COLLOCATIONS POINTS AT REGULAR INTERVALS
parameters=[i for i in range(len(stored_collocations_points_second))] # the range of the number of points we want to consider
number_of_column=4 # the number of columns in the plot
figure, axis = plt.subplots(math.ceil(len(parameters)/number_of_column),number_of_column)
figure.tight_layout()
grid_indices=[1+i for i in range(math.ceil(len(parameters)/number_of_column)*number_of_column)]
#print(grid_indices)
#print(parameters)
#print(axis.flatten())

for par, ax in zip(parameters, axis.flatten()[:len(parameters)]):
    ax.scatter(stored_collocations_points_second[par][:,1],stored_collocations_points_second[par][:,0], c='k',marker="o")
    if par==0:
        ax.set_title("Epoch=1")
    else:
        ax.set_title("Epoch={}".format(par*resample_period))
    
for i in range(len(grid_indices)-len(parameters)):
    axis.flatten()[-1-i].set_visible(False) 
figure.subplots_adjust(top=0.83)
figure.suptitle("Plots of the collocation points at each resampling with $f^\prime$. \n learning_rate={}, number_of_points={}".format(learning_rate,number_of_points), fontsize=20)
plt.show()

 """







# RUN THE THREE NEURAL NETWORKS
learning_rate=1e-3
epochs=20000
number_of_points=400
resample_period=1000
kappa=1/2
c=0
x_domain=np.linspace(0, 1, 100)
y_domain=np.linspace(0, 1, 100)

analytical_pred_matrix = np.array([analytical_solution_x_y(x,y) for x in x_domain for y in y_domain]).reshape(len(x_domain),len(y_domain))



start_grid = time.time()
net_grid = Net(input_dim=2,output_dim=1)
losses_grid, stored_collocations_points_grid, L2_error_grid=fit_grid(model=net_grid, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs)
end_grid = time.time()

time_grid = round(end_grid - start_grid,1)

pinn_preds_grid = predict(net_grid, x_domain,y_domain)
pinn_preds_grid_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_grid_matrix[i][j]=(pinn_preds_grid[i*(len(y_domain))+j].item())

difference_matrix_grid=np.abs(analytical_pred_matrix-pinn_preds_grid_matrix)



start_unif = time.time()
net_unif = Net(input_dim=2,output_dim=1)
losses_unif, stored_collocations_points_unif, L2_error_unif=fit_unif(model=net_unif, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, resample_period=resample_period)
end_unif = time.time()

time_unif = round(end_unif - start_unif,1)

pinn_preds_unif = predict(net_unif, x_domain,y_domain)
pinn_preds_unif_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_unif_matrix[i][j]=(pinn_preds_unif[i*(len(y_domain))+j].item())

difference_matrix_unif=np.abs(analytical_pred_matrix-pinn_preds_unif_matrix)



start_zero = time.time()
net_zero = Net(input_dim=2,output_dim=1)
losses_zero, stored_collocations_points_zero, L2_error_zero=fit_adapt_zero(model=net_zero, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_zero = time.time()

time_zero = round(end_zero - start_zero,1)

pinn_preds_zero = predict(net_zero, x_domain,y_domain)
pinn_preds_zero_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_zero_matrix[i][j]=(pinn_preds_zero[i*(len(y_domain))+j].item())

difference_matrix_zero=np.abs(analytical_pred_matrix-pinn_preds_zero_matrix)



start_prime = time.time()
net_prime = Net(input_dim=2,output_dim=1)
losses_prime, stored_collocations_points_prime, L2_error_prime=fit_adapt_prime(model=net_prime, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_prime = time.time()

time_prime = round(end_prime - start_prime,1)

pinn_preds_prime = predict(net_prime, x_domain,y_domain)
pinn_preds_prime_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_prime_matrix[i][j]=(pinn_preds_prime[i*(len(y_domain))+j].item())

difference_matrix_prime=np.abs(analytical_pred_matrix-pinn_preds_prime_matrix)



start_second = time.time()
net_second = Net(input_dim=2,output_dim=1)
losses_second, stored_collocations_points_second, L2_error_second=fit_adapt_second(model=net_second, x_domain=x_domain, y_domain=y_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_second = time.time()

time_second = round(end_second - start_second,1)

pinn_preds_second = predict(net_second, x_domain,y_domain)
pinn_preds_second_matrix=np.zeros((len(x_domain), len(y_domain)))
for i in range(len(x_domain)):
    for j in range(len(y_domain)):
        pinn_preds_second_matrix[i][j]=(pinn_preds_second[i*(len(y_domain))+j].item())

difference_matrix_second=np.abs(analytical_pred_matrix-pinn_preds_second_matrix)





print('The L2 error of unif-RAD is : {}'.format(np.mean(difference_matrix_unif.reshape(1,-1)[0]**2)))
print('The L2 error of res-RAD is : {}'.format(np.mean(difference_matrix_zero.reshape(1,-1)[0]**2)))
print('The L2 error of grad-RAD is : {}'.format(np.mean(difference_matrix_prime.reshape(1,-1)[0]**2)))
print('The L2 error of hessian-RAD is : {}'.format(np.mean(difference_matrix_second.reshape(1,-1)[0]**2)))








#PLOT THE OUTPUT OF THE PINN FOR THE THREE METHODS AND THE PINN OUTPUTS

max_value_grid=np.max([np.max(l) for l in pinn_preds_grid_matrix])
min_value_grid=np.min([np.min(l) for l in pinn_preds_grid_matrix])

max_value_unif=np.max([np.max(l) for l in pinn_preds_unif_matrix])
min_value_unif=np.min([np.min(l) for l in pinn_preds_unif_matrix])

max_value_zero=np.max([np.max(l) for l in pinn_preds_zero_matrix])
min_value_zero=np.min([np.min(l) for l in pinn_preds_zero_matrix])

max_value_prime=np.max([np.max(l) for l in pinn_preds_prime_matrix])
min_value_prime=np.min([np.min(l) for l in pinn_preds_prime_matrix])

max_value_second=np.max([np.max(l) for l in pinn_preds_second_matrix])
min_value_second=np.min([np.min(l) for l in pinn_preds_second_matrix])

min_output=min([min_value_grid, min_value_unif, min_value_zero, min_value_prime, min_value_second])
max_output=max([max_value_grid, max_value_unif, max_value_zero, max_value_prime, min_value_second])



fig = plt.figure()
gs = gridspec.GridSpec(2, 3)

ax5=plt.subplot(gs[0,0])
im5=ax5.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax5.set_xlabel('x')
ax5.set_ylabel('y')
plt.title('Analytical solution')
#plt.colorbar(im1, ax=ax1) 
divider = make_axes_locatable(ax5)
cax5 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im5, cax = cax5)

ax0=plt.subplot(gs[0,1])
im0=ax0.imshow(pinn_preds_grid_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax0.set_xlabel('x')
ax0.set_ylabel('y')
plt.title('Output of grid-RAD')
#plt.colorbar(im0, ax=ax0) 
divider = make_axes_locatable(ax0)
cax0 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im0, cax = cax0)

ax1=plt.subplot(gs[0,2])
im1=ax1.imshow(pinn_preds_unif_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Output of unif-RAD')
#plt.colorbar(im1, ax=ax1) 
divider = make_axes_locatable(ax1)
cax1 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im1, cax = cax1)

ax2=plt.subplot(gs[1,0])
im2=ax2.imshow(pinn_preds_zero_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Output of res-RAD')
#plt.colorbar(im2, ax=ax2)
divider = make_axes_locatable(ax2)
cax2 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im2, cax = cax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(pinn_preds_prime_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Output of grad-RAD')
#plt.colorbar(im3, ax=ax3)
divider = make_axes_locatable(ax3)
cax3 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im3, cax = cax3)

ax4=plt.subplot(gs[1,2])
im4=ax4.imshow(pinn_preds_second_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='viridis', vmin=min_output, vmax=max_output)
ax4.set_xlabel('x')
ax4.set_ylabel('y')
plt.title('Output of hessian-RAD')
#plt.colorbar(im4, ax=ax4)
divider = make_axes_locatable(ax4)
cax4 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im4, cax = cax4) 

plt.subplots_adjust(wspace = 0.3)
plt.subplots_adjust(hspace = 0.3)
fig.subplots_adjust(top=0.8)

fig.suptitle('Settings: learning_rate={}, nb points={}, epochs={}, tau={}, \n c={}, resample_period={}'.format(learning_rate, number_of_points, epochs, kappa, c, resample_period), fontsize=20, y=0.9)




#PLOT THE L2 ERROR OF THE PINN FOR THE THREE METHODS AND THE PINN OUTPUTS

max_value_diff_grid=np.max([np.max(l) for l in difference_matrix_grid])
min_value_diff_grid=np.min([np.min(l) for l in difference_matrix_grid])

max_value_diff_unif=np.max([np.max(l) for l in difference_matrix_unif])
min_value_diff_unif=np.min([np.min(l) for l in difference_matrix_unif])

max_value_diff_zero=np.max([np.max(l) for l in difference_matrix_zero])
min_value_diff_zero=np.min([np.min(l) for l in difference_matrix_zero]) 

max_value_diff_prime=np.max([np.max(l) for l in difference_matrix_prime])
min_value_diff_prime=np.min([np.min(l) for l in difference_matrix_prime])

max_value_diff_second=np.max([np.max(l) for l in difference_matrix_second])
min_value_diff_second=np.min([np.min(l) for l in difference_matrix_second])

min_heatmap=min([min_value_diff_grid, min_value_diff_unif, min_value_diff_zero, min_value_diff_prime, min_value_diff_second])
max_heatmap=max([max_value_diff_grid, max_value_diff_unif, max_value_diff_zero, max_value_diff_prime, min_value_diff_second])



fig = plt.figure()
gs = gridspec.GridSpec(2, 3)

ax5=plt.subplot(gs[0,0])
im5=ax5.imshow(difference_matrix_grid.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax5.set_xlabel('x')
ax5.set_ylabel('y')
plt.title('Absolute error of grid-RAD')
#plt.colorbar(im5, ax=ax5)
divider = make_axes_locatable(ax5)
cax5 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im5, cax = cax5) 

ax1=plt.subplot(gs[0,1])
im1=ax1.imshow(difference_matrix_unif.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
plt.title('Absolute error of unif-RAD')
#plt.colorbar(im1, ax=ax1) 
divider = make_axes_locatable(ax1)
cax1 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im1, cax = cax1)


ax2=plt.subplot(gs[0,2])
im2=ax2.imshow(difference_matrix_zero.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax2.set_xlabel('x')
ax2.set_ylabel('y')
plt.title('Absolute error of res-RAD')
#plt.colorbar(im2, ax=ax2)
divider = make_axes_locatable(ax2)
cax2 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im2, cax = cax2) 

ax3=plt.subplot(gs[1,1])
im3=ax3.imshow(difference_matrix_prime.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax3.set_xlabel('x')
ax3.set_ylabel('y')
plt.title('Absolute error of grad-RAD')
#plt.colorbar(im3, ax=ax3)
divider = make_axes_locatable(ax3)
cax3 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im3, cax = cax3)

ax4=plt.subplot(gs[1,2])
im4=ax4.imshow(difference_matrix_second.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),y_domain[0]-(y_domain[-1]-y_domain[0])/(2*len(y_domain)),y_domain[-1]+(y_domain[-1]-y_domain[0])/(2*len(y_domain))],aspect='equal',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax4.set_xlabel('x')
ax4.set_ylabel('y')
plt.title('Absolute error of hessian-RAD')
#plt.colorbar(im4, ax=ax4)
divider = make_axes_locatable(ax4)
cax4 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im3, cax = cax4) 

ax6=plt.subplot(gs[1,0])
plt.plot(np.linspace(1, len(L2_error_unif), len(L2_error_unif))[0::100], L2_error_unif[0::100], alpha=0.8,c='b',label='unif-RAD, t={} s'.format(time_unif)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_grid), len(L2_error_grid))[0::100], L2_error_grid[0::100], alpha=0.8,c='k',label='grid-RAD, t={} s'.format(time_grid)) # prediction of the PINN neural network
#plt.plot(np.linspace(1, len(L2_error_rar), len(L2_error_rar))[0::100], L2_error_rar[0::100], alpha=0.8,c='brown',label='RAR sampling') # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_zero), len(L2_error_zero))[0::100], L2_error_zero[0::100], alpha=0.8,c='orange',label="res-RAD, t={} s".format(time_zero)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[0::100], L2_error_prime[0::100], alpha=0.8,c='r',label="grad-RAD, t={} s".format(time_prime)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_second), len(L2_error_second))[0::100], L2_error_second[0::100], alpha=0.8,c='g',label="hessian-RAD, t={} s".format(time_second)) # prediction of the PINN neural network
#plt.ylabel('Temperature (C)')
plt.xlabel('Epochs')
plt.legend(loc="upper right", prop={'size': 13.8})
ax6.set_box_aspect(1)
plt.title('$L_2$-error between the ground truth \n and the PINN outputs during training')

plt.subplots_adjust(wspace = 0.1)
plt.subplots_adjust(hspace = 0.4)
fig.subplots_adjust(top=0.8)

fig.suptitle('Settings: learning_rate={}, nb points={}, epochs={}, tau={}, \n c={}, resample_period={}'.format(learning_rate, number_of_points, epochs, kappa, c, resample_period), fontsize=20, y=0.9)
plt.show()

