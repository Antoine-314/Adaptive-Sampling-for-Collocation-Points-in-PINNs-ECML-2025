# THE EQUATION IS -\Delta(u)=f WITH u(x,y) AS THE FUNCTION


import torch
import torch.nn as nn
import torch.optim as optim
import functools
import matplotlib as mpl ; mpl.rcParams['text.usetex'] = True # to use latex rendering in the legends and titles
import matplotlib.pyplot as plt
import numpy as np
import torch
import math
from numpy import sqrt, sin, cos, pi
import matplotlib.gridspec as gridspec
import time
import copy
from mpl_toolkits.axes_grid1 import make_axes_locatable

torch.manual_seed(42)
np.random.seed(10)




# DEFINE THE GROUND TRUTH FUNCTION
def initial_solution(x:np.array):
    S=[]
    for i in range(1,5):
        S.append(np.sin(i*x)/i)
    return sum(S)+np.sin(8*x)/8

def initial_solution_tensor(Z:torch.Tensor):
    S=[]
    for i in range(1,5):
        S.append(torch.sin(i*Z)/i)
    return sum(S)+torch.sin(8*Z)/8

def analytical_solution(Z:np.array):
    return np.exp(-Z[1])*initial_solution(Z[0])

def analytical_solution_x_t(x:np.array,t:np.array):
    return np.exp(-t)*initial_solution(x)

def R(z:torch.Tensor):
    return torch.exp(-z[:,1].reshape(-1,1))*(3*torch.sin(2*z[:,0].reshape(-1,1))/2+8*torch.sin(3*z[:,0].reshape(-1,1))/3+15*torch.sin(4*z[:,0].reshape(-1,1))/4+63*torch.sin(8*z[:,0].reshape(-1,1))/8)


# CREATE A FUNCTION TO TURN TWO LISTS INTO A GRID
def make_grid(x:np.array, y:np.array):
    return np.array(np.meshgrid(x, y)).T.reshape(-1,2)


# CREATE A FUNCTION TO TURN A 2D LIST INTO TORCH TENSOR
def np_to_th(z:np.array):
    n_samples = z.shape[0]
    return torch.from_numpy(z).to(torch.float64).reshape(n_samples, -1).requires_grad_(True) # float64 is important to get enough precision when we use torch.sin(x) for x close to \pm pi


# CONSTRUCT THE NEURAL NETWORK
torch.set_default_dtype(torch.float64) # set the default torch tensor to be float64, needed for the precision mentioned in np_to_th
class Net(nn.Module):
    def __init__(
        self,
        input_dim, # dimension of the input layer
        output_dim, # dimension of the output layer
        n_units=20, # number of neurones in each hidden layer
    ) -> None:    
        super().__init__()   
        self.n_units = n_units
        self.layers = nn.Sequential(
            nn.Linear(input_dim, self.n_units),
            nn.Tanh(),                                # we use tanh as we will need second derivatives of the network output, and the second derivative of ReLU is zero
            nn.Linear(self.n_units, self.n_units),
            nn.Tanh(),
            nn.Linear(self.n_units, self.n_units),
            nn.Tanh(),
            #nn.Linear(self.n_units, self.n_units),
            #nn.Tanh(),
        )
        self.out = nn.Linear(self.n_units, output_dim)

    def forward(self, input): # this defines the forward pass      input=(x,t)
        h = self.layers(input) # passes the inputs x and t through the hidden layers
        out = self.out(h) # passes the result of the hidden layers through the output layer
        
        return ((input[:,0]-math.pi)*(input[:,0]+math.pi)*input[:,1]*(out.squeeze())+initial_solution_tensor(input[:,0])).squeeze().reshape(-1,1)
        #return input[:,0].reshape(-1,1)*(1-input[:,0].reshape(-1,1))*input[:,1].reshape(-1,1)*(1-input[:,1].reshape(-1,1))*out


def predict(model, x,y): # predict on the fitted model
        model.eval() # model.eval() tells your model that you are evaluating the model.
        out = model(np_to_th(make_grid(x,y)))
        return out.detach().numpy() 


# COMPUTE THE DERIVATIVE
def grad(outputs: torch.Tensor, inputs: torch.Tensor): # Computes the partial derivative of an output with respect to an input
    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True) 


# DEFINE THE LOSS FUNCTION FOR THE PINN WITH MEAN SQUARE ERROR
def physics_loss_mean(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    du_dt = grad(u, evaluation_points)[0][:,1]
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    PDE = d2u_dx2+R(evaluation_points).squeeze()-du_dt # we use .squeeze() on R for the same reason as above
    
    return torch.mean(PDE**2)  # takes the mean of the square of the residue of the equation

   
# DEFINE THE TRAINING PROCEDURE FOR THE RANDOM UNIFORM METHOD
def fit_unif(model: torch.nn.Module, x_domain, t_domain, number_of_points, learning_rate, epochs, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,t_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
    
    losses_total = []
    L2_error=[]
    stored_collocations_points=[]

    for ep in range(1,epochs+1):
        if ep==1 or ep % resample_period ==0:
            evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2)) # we selected randomly and uniformely number_of_points in the domain
            stored_collocations_points.append(evaluation_points)

            evaluation_points_tensor=np_to_th(evaluation_points)

        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
       
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
        loss = physics_loss_mean(model, evaluation_points_tensor) # compute the loss on the evaluation points
        loss.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters of the model by subtracting the gradient 

    return losses_total, stored_collocations_points, L2_error






# DEFINE THE ZERO DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def physics_loss_zero(model: torch.nn.Module, evaluation_points: torch.Tensor): # this defines the loss function of the PINN
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    du_dt = grad(u, evaluation_points)[0][:,1]
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    PDE = d2u_dx2+R(evaluation_points).squeeze()-du_dt # we use .squeeze() on R for the same reason as above

    return (PDE**2).detach().numpy()


# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE ZERO DERIVATIVE
def fit_adapt_zero(model: torch.nn.Module, x_domain, t_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,t_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    stored_collocations_points=[]

    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(physics_loss_zero(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        
    return losses_total,stored_collocations_points,L2_error



# DEFINE THE NORM OF THE DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): 
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    du_dt = grad(u, evaluation_points)[0][:,1]
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    PDE = d2u_dx2+R(evaluation_points).squeeze()-du_dt # we use .squeeze() on R for the same reason as above
    
    gradient_x=grad(torch.mean(PDE**2),evaluation_points)[0][:,0]
    gradient_t=grad(torch.mean(PDE**2),evaluation_points)[0][:,1]

    return torch.sqrt(gradient_x**2+gradient_t**2).detach().numpy()
    

# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE FIRST DERIVATIVE
def fit_adapt_prime(model: torch.nn.Module, x_domain, t_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,t_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    stored_collocations_points=[]

    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(deriv_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        
    return losses_total,stored_collocations_points,L2_error
    


# DEFINE THE NORM OF THE SECOND DERIVATIVE OF THE LOSS FUNCTION FOR THE PINN
def deriv_sec_physics_loss(model: torch.nn.Module, evaluation_points: torch.Tensor): 
    u = model(evaluation_points).squeeze() # we use squeeze because u.size()=torch.Size([10, 1]), but the derivative are torch.Size([10]), so the .squeeze() removes the column
    
    du_dt = grad(u, evaluation_points)[0][:,1]
    d2u_dx2=grad(grad(u, evaluation_points)[0][:,0],evaluation_points)[0][:,0]
    PDE = d2u_dx2+R(evaluation_points).squeeze()-du_dt # we use .squeeze() on R for the same reason as above

    dPDE2_dx=grad(torch.mean(PDE**2),evaluation_points)[0][:,0]
    dPDE2_dt=grad(torch.mean(PDE**2),evaluation_points)[0][:,1]
    d2PDE2_dx2=grad(dPDE2_dx,evaluation_points)[0][:,0]
    d2PDE2_dxdt=grad(dPDE2_dx,evaluation_points)[0][:,1]
    d2PDE2_dtdx=grad(dPDE2_dt,evaluation_points)[0][:,0]
    d2PDE2_dt2=grad(dPDE2_dt,evaluation_points)[0][:,1]

    return torch.sqrt(d2PDE2_dx2**2+d2PDE2_dt2**2+d2PDE2_dxdt**2+d2PDE2_dtdx**2).detach().numpy()
    


# DEFINE THE ADAPTATIVE TRAINING PROCEDURE WITH THE SECOND DERIVATIVE
def fit_adapt_second(model: torch.nn.Module, x_domain, t_domain, learning_rate, epochs, number_of_points, kappa, c, resample_period): # this defines the training phase of the network
    domain_points=make_grid(x_domain,t_domain)
    domain_tensor=np_to_th(domain_points)
    model_2=copy.deepcopy(model)
    #evaluation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
    #evaluation_points_tensor=np_to_th(evaluation_points)

    optimiser = optim.Adam(model.parameters(), lr=learning_rate)

    model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.
        
    losses_total = []
    L2_error=[]
    stored_collocations_points=[]

    for ep in range(1,epochs+1):
        if ep==1:
            collocation_points = np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(number_of_points,2))
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)

        elif ep % resample_period==0:
            # COMPUTE THE NUMBER OF POINTS PER INTERVAL CENTERED ON EACH SEED
            optimiser.zero_grad() # Sets gradients of all model parameters to zero because PyTorch accumulates the gradients on subsequent backward passes
            total_points=np.random.uniform(low=[x_domain[0],t_domain[0]], high=[x_domain[-1],t_domain[-1]], size=(100*number_of_points,2))
            weights=np.abs(deriv_sec_physics_loss(model, np_to_th(total_points)))**kappa
            weights_normed=weights/sum(weights)
            weights_c=weights_normed+[c]*len(weights_normed)
            probabilities=weights_c/sum(weights_c)
            chosen_points=np.random.multinomial(number_of_points, probabilities, size=1)[0]
            indices_of_chosen_points=[i for i, v in enumerate(chosen_points) if v != 0]
            collocation_points=np.array([total_points[i] for i in indices_of_chosen_points])
            stored_collocations_points.append(collocation_points)

            collocation_points_tensor=np_to_th(collocation_points)
            
        model_2.load_state_dict(model.state_dict())
        model_2.zero_grad(set_to_none=True)
        L2_error.append(torch.mean((model_2(domain_tensor)-np_to_th(np.array([analytical_solution(Z) for Z in domain_points])))**2).item())
        losses_total.append(physics_loss_mean(model_2, domain_tensor).item()) # save the loss value on the whole domain

        optimiser.zero_grad() 
        loss_value = physics_loss_mean(model, collocation_points_tensor) # compute the loss on the collocation points
        loss_value.backward() # Compute gradient of the loss w.r.t. to the parameters 
        optimiser.step() # Modify the parameters by subtracting the gradient 
        
    return losses_total,stored_collocations_points,L2_error
    





# RUN THE FOUR NEURAL NETWORKS
learning_rate=1e-4
epochs=100000
number_of_points=50
resample_period=1000
kappa=1/2
c=0
x_domain=np.linspace(-math.pi, math.pi, 100)
t_domain=np.linspace(0, 1, 100)

analytical_pred_matrix = np.array([analytical_solution_x_t(x,t) for x in x_domain for t in t_domain]).reshape(len(x_domain),len(t_domain))


start_unif = time.time()
net_unif = Net(input_dim=2,output_dim=1)
losses_unif, stored_collocations_points_unif, L2_error_unif=fit_unif(model=net_unif, x_domain=x_domain, t_domain=t_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, resample_period=resample_period)
end_unif = time.time()
length_unif = round(end_unif - start_unif,1)
pinn_preds_unif = predict(net_unif, x_domain,t_domain)
pinn_preds_unif_matrix=np.zeros((len(x_domain), len(t_domain)))
for i in range(len(x_domain)):
    for j in range(len(t_domain)):
        pinn_preds_unif_matrix[i][j]=(pinn_preds_unif[i*(len(t_domain))+j].item())
difference_matrix_unif=np.abs(analytical_pred_matrix-pinn_preds_unif_matrix)
max_value_diff_unif=np.max([np.max(l) for l in difference_matrix_unif])
min_value_diff_unif=np.min([np.min(l) for l in difference_matrix_unif])

start_zero = time.time()
net_zero = Net(input_dim=2,output_dim=1)
losses_zero, stored_collocations_points_zero, L2_error_zero=fit_adapt_zero(model=net_zero, x_domain=x_domain, t_domain=t_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_zero = time.time()
length_zero = round(end_zero - start_zero,1)
pinn_preds_zero = predict(net_zero, x_domain,t_domain)
pinn_preds_zero_matrix=np.zeros((len(x_domain), len(t_domain)))
for i in range(len(x_domain)):
    for j in range(len(t_domain)):
        pinn_preds_zero_matrix[i][j]=(pinn_preds_zero[i*(len(t_domain))+j].item())
difference_matrix_zero=np.abs(analytical_pred_matrix-pinn_preds_zero_matrix)
max_value_diff_zero=np.max([np.max(l) for l in difference_matrix_zero])
min_value_diff_zero=np.min([np.min(l) for l in difference_matrix_zero])

start_prime = time.time()
net_prime = Net(input_dim=2,output_dim=1)
losses_prime, stored_collocations_points_prime, L2_error_prime=fit_adapt_prime(model=net_prime, x_domain=x_domain, t_domain=t_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_prime = time.time()
length_prime = round(end_prime - start_prime,1)
pinn_preds_prime = predict(net_prime, x_domain,t_domain)
pinn_preds_prime_matrix=np.zeros((len(x_domain), len(t_domain)))
for i in range(len(x_domain)):
    for j in range(len(t_domain)):
        pinn_preds_prime_matrix[i][j]=(pinn_preds_prime[i*(len(t_domain))+j].item())
difference_matrix_prime=np.abs(analytical_pred_matrix-pinn_preds_prime_matrix)
max_value_diff_prime=np.max([np.max(l) for l in difference_matrix_prime])
min_value_diff_prime=np.min([np.min(l) for l in difference_matrix_prime])

start_second = time.time()
net_second = Net(input_dim=2,output_dim=1)
losses_second, stored_collocations_points_second, L2_error_second=fit_adapt_second(model=net_second, x_domain=x_domain, t_domain=t_domain, number_of_points=number_of_points, learning_rate=learning_rate, epochs=epochs, kappa=kappa, c=c, resample_period=resample_period)
end_second = time.time()
length_second = round(end_second - start_second,1)
pinn_preds_second = predict(net_second, x_domain,t_domain)
pinn_preds_second_matrix=np.zeros((len(x_domain), len(t_domain)))
for i in range(len(x_domain)):
    for j in range(len(t_domain)):
        pinn_preds_second_matrix[i][j]=(pinn_preds_second[i*(len(t_domain))+j].item())
difference_matrix_second=np.abs(analytical_pred_matrix-pinn_preds_second_matrix)
max_value_diff_second=np.max([np.max(l) for l in difference_matrix_second])
min_value_diff_second=np.min([np.min(l) for l in difference_matrix_second])


print('The L2 error of unif-RAD is : {}'.format(np.mean(difference_matrix_unif.reshape(1,-1)[0]**2)))
print('The L2 error of res-RAD is : {}'.format(np.mean(difference_matrix_zero.reshape(1,-1)[0]**2)))
print('The L2 error of grad-RAD is : {}'.format(np.mean(difference_matrix_prime.reshape(1,-1)[0]**2)))
print('The L2 error of hessian-RAD is : {}'.format(np.mean(difference_matrix_second.reshape(1,-1)[0]**2)))




#PLOT THE OUTPUT OF THE PINN FOR THE FOUR METHODS

max_value_unif=np.max([np.max(l) for l in pinn_preds_unif_matrix])
min_value_unif=np.min([np.min(l) for l in pinn_preds_unif_matrix])

max_value_zero=np.max([np.max(l) for l in pinn_preds_zero_matrix])
min_value_zero=np.min([np.min(l) for l in pinn_preds_zero_matrix])

max_value_prime=np.max([np.max(l) for l in pinn_preds_prime_matrix])
min_value_prime=np.min([np.min(l) for l in pinn_preds_prime_matrix])

max_value_second=np.max([np.max(l) for l in pinn_preds_second_matrix])
min_value_second=np.min([np.min(l) for l in pinn_preds_second_matrix])

min_output=min([min_value_unif, min_value_zero, min_value_prime, min_value_second])
max_output=max([max_value_unif, max_value_zero, max_value_prime, min_value_second])


fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[0,0])
im1=ax1.imshow(pinn_preds_unif_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='viridis', vmin=min_output, vmax=max_output)
ax1.set_xlabel('x')
ax1.set_ylabel('t')
plt.title('Output of unif-RAD')
#plt.colorbar(im1, ax=ax1) 
divider = make_axes_locatable(ax1)
cax1 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im1, cax = cax1)

ax2=plt.subplot(gs[0,1])
im2=ax2.imshow(pinn_preds_zero_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='viridis', vmin=min_output, vmax=max_output)
ax2.set_xlabel('x')
ax2.set_ylabel('t')
plt.title('Output of res-RAD')
#plt.colorbar(im2, ax=ax2)
divider = make_axes_locatable(ax2)
cax2 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im2, cax = cax2) 

ax3=plt.subplot(gs[1,0])
im3=ax3.imshow(pinn_preds_prime_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='viridis', vmin=min_output, vmax=max_output)
ax3.set_xlabel('x')
ax3.set_ylabel('t')
plt.title('Output of grad-RAD')
#plt.colorbar(im3, ax=ax3)
divider = make_axes_locatable(ax3)
cax3 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im3, cax = cax3)

ax4=plt.subplot(gs[1,1])
im4=ax4.imshow(pinn_preds_second_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='viridis', vmin=min_output, vmax=max_output)
ax4.set_xlabel('x')
ax4.set_ylabel('t')
plt.title('Output of hessian-RAD')
#plt.colorbar(im4, ax=ax4)
divider = make_axes_locatable(ax4)
cax4 = divider.append_axes("right", size="5%", pad=0.1)
cbar = plt.colorbar(im4, cax = cax4) 

plt.subplots_adjust(wspace = 0.3)
plt.subplots_adjust(hspace = 0.3)
fig.subplots_adjust(top=0.8)

fig.suptitle('Settings: learning_rate={}, nb points={}, epochs={}, tau={}, \n c={}, resample_period={}'.format(learning_rate, number_of_points, epochs, kappa, c, resample_period), fontsize=20, y=0.9)




#PLOT THE HEATMAPS FOR THE FOUR METHODS

min_heatmap=min([min_value_diff_unif, min_value_diff_prime, min_value_diff_second])
max_heatmap=max([max_value_diff_unif, max_value_diff_prime, min_value_diff_second])

fig = plt.figure()
gs = gridspec.GridSpec(2, 2)

ax1=plt.subplot(gs[0,1])
im1=ax1.imshow(difference_matrix_unif.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax1.set_xlabel('x', fontsize=40)
ax1.set_ylabel('t',fontsize=40)
ax1.tick_params(axis='both', which='major', labelsize=40)
plt.title('Erreur de unif-RAD',fontsize=45)
#plt.colorbar(im1, ax=ax1) 
divider = make_axes_locatable(ax1)
cax1 = divider.append_axes("right", size="5%", pad=0.1)
cax1.tick_params(labelsize=26)
cbar = plt.colorbar(im1, cax = cax1)

ax2=plt.subplot(gs[0,0])
im2=ax2.imshow(analytical_pred_matrix.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='viridis')
ax2.set_xlabel('x', fontsize=40)
ax2.set_ylabel('t', fontsize=40)
ax2.tick_params(axis='both', which='major', labelsize=40)
#plt.title('Solution analytique', fontsize=34)
divider = make_axes_locatable(ax2)
cax2 = divider.append_axes("right", size="5%", pad=0.1)
cax2.tick_params(labelsize=40)
cbar = plt.colorbar(im2, cax = cax2)

#ax2=plt.subplot(gs[0,1])
#im2=ax2.imshow(difference_matrix_zero.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
#ax2.set_xlabel('x')
#ax2.set_ylabel('t')
#plt.title('Erreur de res-RAD')
#plt.colorbar(im2, ax=ax2)
#divider = make_axes_locatable(ax2)
#cax2 = divider.append_axes("right", size="5%", pad=0.1)
#cbar = plt.colorbar(im2, cax = cax2) 

ax3=plt.subplot(gs[1,0])
im3=ax3.imshow(difference_matrix_prime.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax3.set_xlabel('x', fontsize=40)
ax3.set_ylabel('t', fontsize=40)
ax3.tick_params(axis='both', which='major', labelsize=40)
plt.title('Erreur de grad-RAD', fontsize=45)
#plt.colorbar(im3, ax=ax3)
divider = make_axes_locatable(ax3)
cax3 = divider.append_axes("right", size="5%", pad=0.1)
cax3.tick_params(labelsize=26)
cbar = plt.colorbar(im3, cax = cax3)

ax4=plt.subplot(gs[1,1])
im4=ax4.imshow(difference_matrix_second.transpose(),origin='lower',extent=[x_domain[0]-(x_domain[-1]-x_domain[0])/(2*len(x_domain)),x_domain[-1]+(x_domain[-1]-x_domain[0])/(2*len(x_domain)),t_domain[0]-(t_domain[-1]-t_domain[0])/(2*len(t_domain)),t_domain[-1]+(t_domain[-1]-t_domain[0])/(2*len(t_domain))],aspect='auto',cmap='plasma', vmin=min_heatmap, vmax=max_heatmap)
ax4.set_xlabel('x', fontsize=40)
ax4.set_ylabel('t', fontsize=40)
ax4.tick_params(axis='both', which='major', labelsize=40)
plt.title('Erreur de hessian-RAD', fontsize=45)
#plt.colorbar(im4, ax=ax4)
divider = make_axes_locatable(ax4)
cax4 = divider.append_axes("right", size="5%", pad=0.1)
cax4.tick_params(labelsize=26)
cbar = plt.colorbar(im3, cax = cax4) 

plt.subplots_adjust(wspace = 0.35)
plt.subplots_adjust(hspace = 0.6)
#fig.subplots_adjust(top=0.8)

#fig.suptitle('Settings: learning_rate={}, nb points={}, epochs={}, tau={}, \n c={}, resample_period={}'.format(learning_rate, number_of_points, epochs, kappa, c, resample_period), fontsize=20, y=0.9)
plt.figure()



#PLOT THE L2 ERROR OF THE PINN FOR THE FOUR METHODS AND THE PINN OUTPUTS

plt.plot(np.linspace(1, len(L2_error_unif), len(L2_error_unif))[0::100], L2_error_unif[0::100], alpha=0.8,c='b',label='unif-RAD, t={} s'.format(length_unif)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_zero), len(L2_error_zero))[0::100], L2_error_zero[0::100], alpha=0.8,c='orange',label="res-RAD, t={} s".format(length_zero)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_prime), len(L2_error_prime))[0::100], L2_error_prime[0::100], alpha=0.8,c='r',label="grad-RAD, t={} s".format(length_prime)) # prediction of the PINN neural network
plt.plot(np.linspace(1, len(L2_error_second), len(L2_error_second))[0::100], L2_error_second[0::100], alpha=0.8,c='g',label="hessian-RAD, t={} s".format(length_second)) # prediction of the PINN neural network
#plt.ylabel('Temperature (Â°C)')
plt.xlabel('Epochs')
plt.legend(loc="upper right", prop={'size': 25})
plt.title('$L_2$-error between the ground truth \n and the PINN outputs during training')
plt.show()
